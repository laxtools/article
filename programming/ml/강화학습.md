# 강화학습 

파이썬과 케라스로 배우는 강화학습 책을 사내 스터디용으로 구매했으나 다른 바쁜 업무가 많아 못 보고 있었다. 이제 리서치를 할 여유가 조금 생겨 다시 보면서 정리하려고 한다. 

# 1장 - 소개

- MDP (Markov Decision Process) 

  - State, Action, Reward, Policy 

- Q Function (큐함수)

  - DQN (Deep Q-Network) 
  - 인공신경망으로 큐 함수를 구현




# 2장 - MDP와 벨만 방정식 

- 그리드월드 
  - 격자 세상에서 길찾기
- 가치함수 
- 벨만 방정식



## MDP

강화학습은 순차적으로 행동을 계속 결정해야 하는 문제를 푸는 것.  따라서, 목표 상태로 가는 과정을 찾는 것. 대부분의 문제는 이런 형태로 변환이 가능하지 않을까? 

- 상태
- 행동
- 보상함수
- 상태변환확률 (State Transition Probability)
- 감가율 (Discount Factor)

MDP는 위로 구성되었다. 

격자세상은 다양한 검색 알고리즘의 구현과 테스트에 간단한 모형을 제공한다. 그리고 세상은 생각보다 격자로 잘 만들 수 있다. ^^

### 상태 

자기가 인식한 상태. 생각의 관점. 

상태 집합, $ S=\{(1,1), (1,2), (1,3), ..., (5,5)\}$

시간 $t $일 때 상태, $S_t=(1,3)$
고정된 값이 아닌 확률에 따라 상태가 결정될 수 있다. 위의 $S_t$는 상태 변수로 본다. 

대문자를 확률 변수로 둔다. 시간 $t$의 상태 $S_t$가 $s$라고 할 때, $S_t=s$와 같이 표현한다. 

$S_t=s$라고 하면 $S_{1:10}=(1,4)$와 같은 값을 갖는다는 뜻이다. 

### 행동 

$S_t$에서 선택 가능한 행동의 집합을 $A$라고 하자. 시간 $t$에 에이전트가 행동 $a$를 했다면 $A_t=a$와 같이 표현한다. 

$A_{1:10}=위로 가기$ 와 같다. $A_t$도 확률변수로 볼 수 있다. 

격자 세상에서 행동 집합은 $A = \{up, down, left, right\}$이다 

동일 행동을 해도 다음 상태로 항상 간다는 보장은 없다. 이는 상태 변환 확률이 결정한다. 

### 보상 함수

시간 $t$에서 상태가 $S_t=s$이고 행동이 $A_t=a$일 때, 보상 함수는 다음과 같다 .

$$ R_s^a = E[R_{t+1}| S_t=s, A_t=a]$$

위의 $E$는 $R_{t+1}$의 기댓값이며 $|$는 조건부 확률을 의미한다. $t+1$ 시간의 보상 값인 이유는 다음 시간에 결과를 알기 때문이다. 조건부 확률 $P(A|B) = P(A \cap B) / P(B)$ 이다. 이미 B가 일어난 상태에서 A일 확률이다. 

위의 식에서 기대값을 계산할 수 있어야 하는데 구체적인 값들로 바꿀 수 있어야 한다. 

시간을 이산적으로 생각한다. discrete simulation과 비슷하게 생각한다. 실제 계산에서는 연속으로 볼 수도 있을 듯 하다. 분포 함수의 연속성에 따라 달라질 듯 하다. 

### 상태 변환 확률

$$P_{ss'}^a = P[S_{t+1}=s' | S_t=s, A_t=a]$$  

상태 s에서 행동 a를 취하고 s'으로 변환될 확률 

### 감가율

시간이 지날수록 보상을 적게 줄이는 비율. 시간에 따른 이자로 생각해도 되고 탐색 공간을 줄이기 위한 방법이라고 봐도 될 듯. 

$$\gamma \in [0, 1]$$ 0과 1 사이의 수치를 감가율로 한다. 

여기서는 고정된 수치로 하고 있다. 

### 정책 

개별 상태에 행동을 선택할 확률 또는 확률 함수. 
$$ \pi (a | s) = P[A_t=a | S_t=s]$$

정책 확률 함수는 $S_t=s$일 때 $A_t=a$를 선택할 확률. 



여기까지 보면 상태에서 정책에 따라 행위를 하고 보상을 받는 과정을 따라 최적의 정책을 찾는 것을 강화학습이라 할 수 있다. 

## 가치함수 

감가율을 적용하여 보상을 계산한 함수 $G_t$를 반환값(return value)이라 하고 아래와 같이 정의한다. 

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... $$ 

반환값이라고 되어 있는데 보상값의 합이므로 수익기대값이라고 봐도 된다. 

각 시간 1, 2, 3, ... 에 대한 $G_1, G_2, G_3, ... $이 책에 나와 있는데 계산할 수 있어야 한다. 

반환값에 대한 기대값이 특성 상태의 보상 기대값이며 가치함수라고 한다. 

$$ v(s) = E[G_t | S_t=s]$$

아래 수식의 전개 과정은 중요해 보인다. 
$$
v(s) = E[R_{t+1} + \gamma R_{t+2} \gamma^2 R_{t+3} ... | S_t = s ]
\\
v(s) = E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+3} ...) | S_t = s]
\\
v(s) = E[R_{t+1} + \gamma G_{t+1} | S_t = s]
\\
v(s) = E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]
$$
좀 더 구체적인 예가 주어질 때 (아니면 만들어서라도) 위의 값을 계산할 수 있어야 한다. 

위에는 정책이 반영되지 않았는데 정책이 반영된 기대값(?)으로 바꾼다. 

$$ v_{\pi}(s) = E_\pi[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s]$$

파이만 밑에 써주면 되는 건 아닌 것 같지만 일단 이 정도로 이해한다. 이 방정식이 **벨만 방정식**이다. 

**강화학습은 벨만 방정식을 어떻게 풀어 나가느냐의 스토리**입니다. 

위의 가치 함수는 "상태" 가치함수이다. 즉, 어떤 상태가 얼마나 좋은 지를 알려준다. 

### 큐 함수 

큐함수는 "행동" 가치함수이다. 어떤 상태에서 어떤 "행동"이 얼마나 좋으냐를 알려준다. 

$q_{\pi}(s, a)$ 로 상태와 행동에서 가치를 매긴다. 

큐함수와 가치함수와 관계를 나타내는 식이다. 

$$ v_{\pi}(s) = \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)$$ 

큐함수도 벨만 기대 방정식으로 나타낼 수 있다. 조건에 행동이 더 들어간다. 

$$ q_{\pi}(s, a) = E_\pi [R_{t+1} + \gamma q_\pi (S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$$  



위의 두 가지 식은 모두 중요해 보인다. 그 의미를 파악하고 사용하는 것이 강화 학습의 핵심으로 보인다. 



